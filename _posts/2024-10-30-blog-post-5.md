---
title: 'DREAM: Efficient Dataset Distillation by Representative learning - Paper Blog Post'
date: 2024-10-29
permalink: /posts/2024/10/blog-post-5/
tags:
  - Cluster Centroid initialization
  - IDC baseline
  - Balanced mini-batch formation
---

DREAM: Efficient Dataset Distillation by Representative learning 
======

# Introduction

## What is Dataset distillation?

Training a network on ImageNet-1k dataset is a very heavy task, requiring multiple epochs of forward and backward pass with 1.28million different image-label pairs. Not only does the training take much time, large dataset cannot be moved to On-Devices because of the memory size. To resolve this problem, Dataset Distillation aims to compress large training dataset into lightweight dataset while maintaining its capability to train models in image classification task. With such replacement training time and energy consumption can be reduced while training can be done on On-Devices with small memory space. To accomplish this, the field of Dataset Distillation synthesizes dataset through backpropagation with variety of objectives in hand.

For clarification, 'synthetic dataset' is the targeted lightweight dataset that is synthesized through backpropagation during dataset distillation procedure. 'Real training dataset' is the actual heavy dataset that requires to be compressed. For example, given that the original CIFAR10 dataset have 5000 images per class, IPC in short, the aim is to reduce its size to 1IPC, 10IPC or 50IPC. Moreover, synthetic images are intialized as Gaussian noise images or subset of real training images.

## Various Objective settings for Dataset Distillation

### 1. Main Objective - Dataset Distillation (2018)

First paper of Dataset Distillation dates back to 2018 with a formal objective. Field of Dataset Distillation aims to make a synthetic dataset that trains network to well classify real images. To match this definition, the objective for training the synthetic image was set to 'minimizing classification loss of real training dataset when it is forwarded on network that is trained on synthetic dataset for multiple iterations'. In this objective setting, updating synthetic dataset with backpropagation is done by computing the gradient of gradient, a Hessian, which is a very unstable optimization problem. Also, the number of iterations synthetic dataset has been used for training determines the complexity of hessian calculation. In order to update the synthetic image, the whole training steps needs to be unrolled and be used for backpropagation. This framework is called 'BackPropagation Through Time (BPTT)' and is why the synthetic image generation takes a very long time.

### 2. Surrogate Objective - Dataset Condensation with Gradient Matching (2021)

Dataset Distillation was forgotten for few years until this new surrogate objective was discovered. Gradient Matching method is based on matching the gradient of synthetic dataset and real training dataset throughout the parametric trajectory when model is being trained. This way the synthetic image will mimic the parametric trajectory of real training image resulting in a similar paramteric space when model is trained on synthetic image, and hence a similar performance to when model is being trained on real training dataset. The update procedure is conducted on multiple randomly intialized models for generalization to intialization in parametric space. Within each model instance the gradient matching is conducted with outer training loop and inner training loop. Outer training loop trains the model on synthetic dataset creating parameteric instances where the gradient matching is conducted until convergence. Inner training loop iterates throughout the classes to match gradient of real and synthetic dataset at each parameteric instance seperately without actual update. This way synthetic image is updated so that it mimic the real dataset's gradient in each of the parameteric instance throughout the trajectory resulting in a similar parametric convergence.

The paper to be reviewed in this blog post is based on this method, in fact a more developed gradient matching method called 'IDC'. There are three distinguishing features between vanilla Gradient Matching method and IDC method. Firstly, the outer training loop is trained on real training dataset to determine the parametric instance where gradient matching is conducted. This way the dependency between the parameteric instance and gradient computation of sythetic image can be mitigated. Also, the training with small synthetic dataset converges at early training and result in the gradient norm to decrease quickly leaving no gradient to be matched. Nextly, IDC condenses downsampled image and restore through bilinear interpolation when used for training. This means that, in 2 factor setting, the size of dataset can be diminished by 4-fold, meaning with previous memory storage of 1IPC, up to 4IPC can be used for training. Since the high frequency component of image is not critical to performance due to natural property of image, 10IPC of this method produces near the performance of 40IPC of previous method. Finally, gradient matching metric is different. Previous method prefers the use of cosine similarity for gradient matching based on empirical results. However, IDC uses MSE distance to match gradients. 

Other than these methods there are Matching Training Trajectory method, distribution matching method, knowledge distillation method and so on.

# Summary

## Problem Setting

Dataset distillation aims to synthesize dataset that represent the whole distribution of real dataset. While original DC method uses random sampled mini-batches of real dataset for gradinet matching, DREAM takles the lack of training efficiency in forming the mini-batch with randomly sampled images. 

Previous methods use randomly sampled real image mini-batch for matching gradient with synthetic image for update at each iteration. With random sampling, each mini-batch sampled is not gaurenteed to cover the whole real dataset distribution evenly, nor consistently at every iteration. This induces each update to the synthetic image to be unstable, requiring excessive iteration for convergence. Moreover, randomly sampled mini-batches may be concentrated near certain part of decision boundary leading to larger gradient norm that serves as a noise to updating the synthetic images. In addition, previous methods intialize synthetic image with randomly sampled real image. Random intialization of synthetic image also does not provide the optimal prior condition for distillation, since it does not reflect the full distribution of the dataset.

## Method

To address this problem, DREAM utilizes the well-known k-means clustering method for forming the mini-batch. This induces stable, and hence fastened optimization of synthetic image without noises. 

### K-means clustering
In general, k-means clustering is used to cluster unlabeled data points that minimize their distance within the groups. It only needs setting of the number of cluster, k. For simple explanation, k number of imaginary data points are generate, what we call centroid, then each data points are grouped to the nearest centroid. Centroid is then moved to the average of the data points in that group, and the process is continued until convergence. 

In vision tasks, k-means clustering can be used for intra-class clustering, clustering feature extractor output of each image in the same class to group them by the distinguished features each images have. A good example of feature extractor would be the embedding function of a pretrained network. For example, some of the images in class plane may show the head of a plane while some of them may show the tail of a plane. Passing these two images through the feature extractor would give distant outputs while images with similar feature would give neighbouring outputs. This way by using k-means clustering on 





![Editing a markdown file for a talk](/Multisize_DC/슬라이드1.PNG)

------
