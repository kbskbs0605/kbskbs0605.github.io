---
title: 'DREAM: Efficient Dataset Distillation by Representative learning - Paper Blog Post'
date: 2024-10-29
permalink: /posts/2024/10/blog-post-5/
tags:
  - Cluster Centroid initialization
  - IDC baseline
  - Balanced mini-batch formation
---

Paper blog post on DREAM: Efficient Dataset Distillation by Representative learning 
======

### What is Dataset distillation?

Training a network on ImageNet-1k dataset is a very heavy task, requiring multiple epochs of forward and backward pass with 1.28million different image-label pairs. Not only does the training take much time, large dataset cannot be moved to On-Devices because of the memory size. To resolve this problem, Dataset Distillation aims to compress large training dataset into lightweight dataset while maintaining its capability to train models in image classification task. With such replacement training time and energy consumption can be reduced while training can be done on On-Devices with small memory space. To accomplish this, the field of Dataset Distillation synthesizes dataset through backpropagation with variety of objectives in hand.

For clarification, 'synthetic dataset' is the targeted lightweight dataset that is synthesized through backpropagation during dataset distillation procedure. 'Real training dataset' is the actual heavy dataset that requires to be compressed.

### Various Objective settings for Dataset Distillation

#### 1. Main Objective - Dataset Distillation(2018)

First paper of Dataset Distillation dates back to 2018 with the most formal objective. Field of Dataset Distillation aims to make a synthetic dataset that trains network to well classify real images. To match this definition, the objective for training the synthetic image was set to 'minimizing classification loss of real training dataset when it is forwarded on network that is trained on synthetic dataset'. In this objective setting, updating synthetic dataset with backpropagation is done by computing the gradient of gradient, a Hessian, which is very unstable optimization problem. Also, depending on the amount of iteration the synthetic dataset will be used for training the network 


![Editing a markdown file for a talk](/Multisize_DC/슬라이드1.PNG)

------
