---
title: 'DREAM: Efficient Dataset Distillation by Representative learning - Paper Blog Post'
date: 2024-10-29
permalink: /posts/2024/10/blog-post-5/
tags:
  - Cluster Centroid initialization
  - IDC baseline
  - Balanced mini-batch formation
---

DREAM: Efficient Dataset Distillation by Representative learning 
======

# Introduction

## What is Dataset distillation?

Training a network on ImageNet-1k dataset is a very heavy task, requiring multiple epochs of forward and backward pass with 1.28million different image-label pairs. Not only does the training take much time, large dataset cannot be moved to On-Devices because of the memory size. To resolve this problem, Dataset Distillation aims to compress large training dataset into lightweight dataset while maintaining its capability to train models in image classification task. With such replacement training time and energy consumption can be reduced while training can be done on On-Devices with small memory space. To accomplish this, the field of Dataset Distillation synthesizes dataset through backpropagation with variety of objectives in hand.

For clarification, 'synthetic dataset' is the targeted lightweight dataset that is synthesized through backpropagation during dataset distillation procedure. 'Real training dataset' is the actual heavy dataset that requires to be compressed. For example, given that the original CIFAR10 dataset have 5000 images per class, IPC in short, the aim is to reduce its size to 1IPC, 10IPC or 50IPC. Moreover, synthetic images are intialized as Gaussian noise images or subset of real training images.

## Various Objective settings for Dataset Distillation

### 1. Main Objective - Dataset Distillation (2018)

First paper of Dataset Distillation dates back to 2018 with a formal objective. Field of Dataset Distillation aims to make a synthetic dataset that trains network to well classify real images. To match this definition, the objective for training the synthetic image was set to 'minimizing classification loss of real training dataset when it is forwarded on network that is trained on synthetic dataset for multiple iterations'. In this objective setting, updating synthetic dataset with backpropagation is done by computing the gradient of gradient, a Hessian, which is a very unstable optimization problem. Also, the number of iterations synthetic dataset has been used for training determines the complexity of hessian calculation. In order to update the synthetic image, the whole training steps needs to be unrolled and be used for backpropagation. This framework is called 'BackPropagation Through Time (BPTT)' and is why the synthetic image generation takes a very long time.

### 2. Surrogate Objective - Dataset Condensation with Gradient Matching (2021)

Dataset Distillation was forgotten for few years until this new surrogate objective was discovered. Gradient Matching method is based on matching the gradient of synthetic dataset and real training dataset throughout the parametric trajectory when model is being trained. This way the synthetic image will mimic the parametric trajectory of real training image resulting in a similar paramteric space when model is trained on synthetic image, and hence a similar performance to when model is being trained on real training dataset. The update procedure is conducted on multiple randomly intialized models for generalization to intialization in parametric space. Within each model instance the gradient matching is conducted with outer training loop and inner training loop. Outer training loop trains the model on synthetic dataset creating parameteric instances where the gradient matching is conducted, in other word creating a parametric trajectory. Inner training loop iterates throughout the classes to match gradient of real and synthetic dataset at each parameteric instance seperately without actual update. This way synthetic image is updated so that it will mimic the real dataset's gradient throughout the trajectory when used for training, eventually resulting in a similar parametric convergence and similar performance.

The paper to be reviewed in this blog post, DREAM, is not entirely based on this method, in fact a more developed gradient matching method called 'IDC'. There are three distinguishing features between naive Gradient Matching method and IDC method. Firstly, the outer training loop is trained on real training dataset batches. This way the dependency between the parameteric instance and gradient computation of sythetic image can be mitigated. Also, for naive gradient matching method the training with small synthetic dataset converges at early training and result in the gradient norm to decrease quickly leaving no gradient to be matched at late stages. Whereas, creating trajectory with real training image creates more parametric instances that are suitable for gradient matching.

 Nextly, IDC condenses downsampled image and resize through bilinear interpolation when used for training. To elaborate, IDC stores synthetic image as downsampled form while when being used for gradient matching it is restored to its original size. This means that, in 2 factor downsampling setting, the size of dataset can be diminished by 4-fold, meaning with previous memory storage of 1IPC, up to 4IPC can be used for training. Since the high frequency component of image is not crucial to performance due to natural property of image, 10IPC of this method produces near the performance of 40IPC of previous method. Finally, gradient matching metric is different. Previous method prefers the use of cosine similarity for gradient matching based on empirical results. However, IDC uses MSE distance to match gradients. 

Other than these methods there are Matching Training Trajectory method, distribution matching method, knowledge distillation method and so on.

# Summary

## Problem Setting

Dataset distillation aims to synthesize dataset that represent the whole distribution of real dataset. While original DC method uses random sampled mini-batches of real dataset for gradinet matching, DREAM takles the lack of training efficiency in forming the mini-batch with randomly sampled images. 

Previous methods use randomly sampled real image mini-batch for matching gradient with synthetic image for update at each iteration. With random sampling, each mini-batch sampled is not gaurenteed to cover the whole real dataset distribution evenly, nor consistently at every iteration. This induces each update to the synthetic image to be unstable, requiring excessive iteration for convergence. Moreover, randomly sampled mini-batches may be concentrated near certain part of decision boundary leading to larger gradient norm that serves as a noise to updating the synthetic images. In addition, previous methods intialize synthetic image with randomly sampled real image. Random intialization of synthetic image also does not provide the optimal prior condition for distillation, since it does not reflect the full distribution of the dataset.

## Method

To address this problem, DREAM utilizes the well-known k-means clustering method for forming the mini-batch. This induces stable, and hence fastened optimization of synthetic image without noises. 

### K-means Clustering
In general, k-means clustering is used to cluster unlabeled data points that minimize their distance within the groups. It only needs setting of the number of cluster, k. For simple explanation, k number of imaginary data points are generate, what we call centroid, then each data points are grouped to the nearest centroid. Centroid is then moved to the average of the data points in that group, and the process is continued until convergence. 

In vision tasks, k-means clustering can be used for intra-class clustering, clustering feature extractor output of each image in the same class. A good example of feature extractor would be the embedding function of a pretrained network to feature space. For example, some of the images in class plane may show the head of a plane while some of them may show the tail of a plane. Passing these two images through the feature extractor would give distant outputs while images with similar feature would give neighbouring outputs. Applying k-means clustering would group the images in that class with similar features. Selecting image from each cluster will well represent the distribution of whole dataset evenly. Simply put, this is how DREAM constitute the mini-batch for updating synthetic image stably and efficiently.

### Training efficiency
DREAM studies varying factors in random sampling that result in optimization instability and degradation of training efficiency.

Firstly, gradients from samples in different regien varies. Samples in the center of distribution induces low gradient norm in the parameteric space since they have high prediction accuracy. Samples at the boundary, on the other hand, induces high gradient norm that would dominate the optimization direction. Gradient matching with supervision form unbalanced minibatch with these extreme samples will cause unstable optimization eventually leading to performance degradation. The paper empirically demonstrate the effect of training with such extreme samples.

(Fig.2b)

Secondly, random sampling for generating mini-batches cannot consistently cover the original dataset distribution of that class. With the following unevenly distributed sample for supervision, boundary samples may cause unstable and large gradient as a matching target. To investigate the phenomenom of uneven distribution in randomly sampled mini-batch, the paper shows the MMD, a distribution distance, of sampled mini-batch and actual real image distribution thorughout the iterations. They compare their method of constituting the mini-batch with random sampling.

(Fig.2c)








![Editing a markdown file for a talk](/Multisize_DC/슬라이드1.PNG)

------
