---
title: 'DREAM: Efficient Dataset Distillation by Representative learning - Paper Blog Post'
date: 2024-10-29
permalink: /posts/2024/10/blog-post-5/
tags:
  - Cluster Centroid initialization
  - IDC baseline
  - Balanced mini-batch formation
---

DREAM: Efficient Dataset Distillation by Representative learning 
======

# Introduction

## What is Dataset distillation?

Training a network on ImageNet-1k dataset is a very heavy task, requiring multiple epochs of forward and backward pass with 1.28million different image-label pairs. Not only does the training take much time, large dataset cannot be moved to On-Devices because of the memory size. To resolve this problem, Dataset Distillation aims to compress large training dataset into lightweight dataset while maintaining its capability to train models in image classification task. With such replacement training time and energy consumption can be reduced while training can be done on On-Devices with small memory space. To accomplish this, the field of Dataset Distillation synthesizes dataset through backpropagation with variety of objectives in hand.

For clarification, 'synthetic dataset' is the targeted lightweight dataset that is synthesized through backpropagation during dataset distillation procedure. 'Real training dataset' is the actual heavy dataset that requires to be compressed. For example, given that the original CIFAR10 dataset have 5000 images per class, IPC in short, the aim is to reduce its size to 1IPC, 10IPC or 50IPC. Moreover, synthetic images are intialized as Gaussian noise images or subset of real training images.

## Various Objective settings for Dataset Distillation

### 1. Main Objective - Dataset Distillation (2018)

First paper of Dataset Distillation dates back to 2018 with a formal objective. Field of Dataset Distillation aims to make a synthetic dataset that trains network to well classify real images. To match this definition, the objective for training the synthetic image was set to 'minimizing classification loss of real training dataset when it is forwarded on network that is trained on synthetic dataset for multiple iterations'. In this objective setting, updating synthetic dataset with backpropagation is done by computing the gradient of gradient, a Hessian, which is a very unstable optimization problem. Also, the number of iterations synthetic dataset has been used for training determines the complexity of hessian calculation. In order to update the synthetic image, the whole training steps needs to be unrolled and be used for backpropagation. This framework is called 'BackPropagation Through Time (BPTT)' and is why the synthetic image generation takes a very long time.

### 2. Surrogate Objective - Dataset Condensation with Gradient Matching (2021)

Dataset Distillation was forgotten for few years until this new surrogate objective was discovered. Gradient Matching method is based on matching the gradient of synthetic dataset and real training dataset throughout the parametric trajectory when model is being trained. This way the synthetic image will mimic the parametric trajectory of real training image resulting in a similar paramteric space when model is trained on synthetic image, and hence a similar performance to when model is being trained on real training dataset. The update procedure is conducted on multiple randomly intialized models for generalization to intialization in parametric space. Within each model instance the gradient matching is conducted with outer training loop and inner training loop. Outer training loop trains the model on synthetic dataset creating parameteric instances where the gradient matching is conducted until convergence. Inner training loop iterates throughout the classes to match gradient of real and synthetic dataset at each parameteric instance seperately without actual update. This way synthetic image is updated so that it mimic the real dataset's gradient in each of the parameteric instance throughout the trajectory resulting in a similar parametric convergence.

The paper to be reviewed in this blog post is based on this method, in fact a more developed gradient matching method called 'IDC'. There are three distinguishing features between vanilla Gradient Matching method and IDC method. Firstly, the outer training loop is trained on real training dataset to determine the parametric instance where gradient matching is conducted. This way the dependency between the parameteric instance and gradient computation of sythetic image can be mitigated. Also, the training with small synthetic dataset converges at early training and result in the gradient norm to decrease quickly leaving no gradient to be matched. Nextly, IDC condenses downsampled image and restore through bilinear interpolation when used for training. This means that, in 2 factor setting, the size of dataset can be diminished by 4-fold, meaning with previous memory storage of 1IPC, up to 4IPC can be used for training. Since the high frequency component of image is not critical to performance due to natural property of image, 10IPC of this method produces near the performance of 40IPC of previous method. Finally, gradient matching metric is different. Previous method prefers the use of cosine similarity for gradient matching based on empirical results. However, IDC uses MSE distance to match gradients. 

Other than these methods there are Matching Training Trajectory method, distribution matching method, knowledge distillation method and so on.

# DREAM: Efficient Dataset Distillation by Representative learning

## Problem setting

DREAM takles the lack of eveness in the real image sample for supervision and intialization. Previous methods use randomly sampled real image mini-batch for matching gradient as supervision to updating synthetic image. Since such mini-batch is not gaurenteed to cover even and diverse distribution of the real training image, the optimization of the synthetic image is unstable, requiring excessive iteration for convergence. Moreover, randomly sampled mini-batches may be concentrated near certain part of decision boundary leading to higher gradient norm that serves as a noise to updating the synthetic images. Random intialization of synthetic image also does not provide optimal prior condition for distillation, since it does not reflect the full distribution of the dataset.

To mitigate this two problem setting, DREAM suggests k-means clustering method prior to distillation process for stable and efficient optimization of synthetic image and full-covering distribution.

## 


![Editing a markdown file for a talk](/Multisize_DC/슬라이드1.PNG)

------
