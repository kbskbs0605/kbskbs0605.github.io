---
title: 'DREAM: Efficient Dataset Distillation by Representative learning - Paper Blog Post'
date: 2024-10-29
permalink: /posts/2024/10/blog-post-5/
tags:
  - Cluster Centroid initialization
  - IDC baseline
  - Balanced mini-batch formation
---

Paper blog post on DREAM: Efficient Dataset Distillation by Representative learning 
======

What is Dataset distillation?

Training a network on ImageNet-1k dataset is a very heavy task, requiring up to 1.28million image-label pairs for forward and backward pass. Not only does the training take much time, large dataset cannot be moved to On-Devices because of the memory size. To resolve this problem Dataset Distillation aims to compress large training dataset into lightweight dataset while maintaining its capability to train models. With such replacement training time and energy consumption can be reduced while training can be done on On-Devices with small memory space. To accomplish this the field of Dataset Distillation synthesizes dataset through backpropagation with distinguished objectives in hand.
![Editing a markdown file for a talk](/Multisize_DC/슬라이드1.PNG)

------
